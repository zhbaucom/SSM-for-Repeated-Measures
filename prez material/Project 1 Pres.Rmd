---
title: "State Space Models with Longitudinal Data"
author: Zach Baucom
output:
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{amsmath}
- \newcommand\mysim{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny asym}}}{\sim}}}
---


```{r, include = FALSE}
library(gridExtra)
library(lme4)
library(tidyverse)
library(knitr)
library(kableExtra)
library(latex2exp)
### Set working directory to correct location
# cdir <- stringr::str_split(getwd(), "/")[[1]]
# udir <- cdir[1:which(cdir == "State-Space-Methods")]
# knitr::opts_knit$set(root.dir = paste(udir, collapse = "/"))

fct_case_when <- function(...) {
  args <- as.list(match.call())
  levels <- sapply(args[-1], function(f) f[[3]])  # extract RHS of formula
  levels <- levels[!is.na(levels)]
  factor(dplyr::case_when(...), levels=levels)
}
knitr::opts_chunk$set(echo = FALSE, dev = "pdf")
```




## Introduction

* State Space Models have been primarily used for time series data with a large number of time points and only a small number of chains observed.
* We are working to apply these models to a small number of time points and a large number of subjects.
  * Small $t$ and large $n$ are typically what we see in observational data.
* We wish to show that the State Space Model can be more flexible and robust than the commonly used mixed effect models (Laird and
Ware, 1983; Diggle, Liang and Zeger, 1994).

## Computation Consideration

* State space models can be computationally intensive.
* We will compare different state space model estimation methods to find the best balance of computational efficiency and accuracy.
  * State space model in matrix form.
  * Partitioned state space model.
  * Bayesian state space model.

## State Space Model

A general linear state space model can be denoted as:


\begin{align*}
y_t = F_t\mu_t + v_t\\
\mu_t = G_t\mu_{t-1} + w_t
\end{align*}

where at time $t$,

* $y_t$ is the an $n \times 1$ observation vector.
* $\mu_t$ is the $q \times 1$ latent state vector, where $q$ is the number of latent states.
* $F_t$ is the $n \times q$ observation matrix.
* $G_t$ is the $q \times q$ state transition matrix.

We assume $v_t$ and $w_t$ are independent identically distributed with distributions $v_t \sim N(0, V)$ and $w_t \sim N(0,W)$ respectively (Harvey, 1990; Durbin and Koopman, 2012)
.


## State Space Model Illustration

General Model:

\begin{align*}
y_t = F_t\mu_t + v_t\\
\mu_t = G_t\mu_{t-1} + w_t
\end{align*}

![](images/hmm.jpg)


## Proposed Model

We wish to model the data according to the following,

\begin{align*}
y_t &= \alpha_t + X_t \beta_t
+ \varepsilon_t\\
\alpha_t &= \alpha_{t-1} + \eta_t\\
\beta_t &= \beta_{t-1}
\end{align*}

Where $\alpha_0 \sim N(a_0, P_0)$, $\beta_0 \sim N(\beta, 0)$, $\varepsilon_t \sim N(0, \sigma^2_\varepsilon I_n)$, and $\eta_t \sim N(0, \sigma^2_\eta I_n)$.

* $y_t$ is an $n \times 1$ observation vector where $n$ indicates the number of subjects.
* $\alpha_t$ is an $n \times 1$ latent state vector.
  * Variation in $\alpha_t$ over time creates a dynamic moving average auto-correlation between observations $y_t$.
* $X_t$ is an $n \times p$ matrix of time varying covariates (can be $X_t = t * X$ where $X$ are baseline covarties).
* $\beta_t$ is the linear effect of the columns in $X_t$.



## What is $\alpha_t$

Consider the model,

\begin{align*}
y_t &= \alpha_t + X_t\beta_t
+ \varepsilon_t\\
\alpha_t &= \alpha_{t-1} + \eta_t\\
\beta_t &= \beta_{t-1}
\end{align*}

We can think of $\alpha_t$ as the underlying cognitive state not accounted for by the baseline covariates $X$. 

Notice $\alpha_t|\alpha_{t-1} \sim N(\alpha_{t-1}, \sigma^2_\eta)$. This means our next underlying cognitive state will be centered at the previous underlying cognitive state.

Remember $E(\alpha_t) = E(\alpha_{t-1} + \eta_t) = E(\alpha_{t-1})$. If we iterate all the way down $E(\alpha_t) = a_0$. So $\alpha_t > \alpha_0$ represents an up phase and $\alpha_t < \alpha_0$ represents a down phase.


## LME with Random Intercept

Consider the model: $y_{it} = b_{i0} + t * \beta + \epsilon_{it}$ where $b_{i0}\sim  iid  \ N(0, \sigma^2_b)$ and $\epsilon_{it} \sim iid \ N(0, \sigma^2)$.

Let $\beta = 1$, $\sigma^2_b = 10$, and $\sigma^2 = 1$.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height=5}
set.seed(13)

n <- 3

ri <- rnorm(n, sd = sqrt(20))

y <- sapply(1:10, function(x){
  x + rnorm(n)
}) + ri



p <- y %>% 
  as_tibble() %>%
  mutate(id = as.character(1:n)) %>%
  gather("time", "y",V1:V10) %>%
  mutate(time = as.numeric(substr(time, 2, nchar(time)))) %>%
  left_join(
    tibble(id = as.character(1:n), ri = ri), by = "id"
  ) %>%
  mutate(y.hat = ri + time) %>%
  ggplot(aes(x = time, y = y)) + 
  geom_point()
p + geom_smooth(method = "lm", size = 2, color = "black", se = FALSE)

```


## LME with Random Intercept

Consider the model: $y_{it} = b_{i0} + t * \beta + \epsilon_{it}$ where $b_{i0}\sim  iid  \ N(0, \sigma^2_b)$ and $\epsilon_{it} \sim iid \ N(0, \sigma^2)$.

Let $\beta = 1$, $\sigma^2_b = 10$, and $\sigma^2 = 1$.

```{r, echo = FALSE, fig.height=5}

p +
  geom_point(aes(color = id)) +
  geom_line(aes(y = y.hat, color = id))


```

## Single observation from a LMEM

```{r, echo = FALSE, message = FALSE, fig.height=6}
set.seed(13)

n <- 1
t <- 100
y <- sapply(1:t, function(x){
  x + rnorm(n, sd = 5)
}) 

yd <- tibble(y, time = 1:t)

p1 <- ggplot(yd, aes(x = time, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

yd$resid <- resid(lm(y ~ time, data = yd))

p2 <- ggplot(yd, aes(x = time, y = resid)) + 
  geom_point() + geom_hline(yintercept = 0, color = "red")

grid.arrange(p1, p2, nrow = 1)

```

## Single observation from a SSM

```{r, echo = FALSE, message = FALSE, fig.height=6}
set.seed(1)

n <- 1
t <- 100
y <- numeric(t)
mu <- numeric(t)
for(i in 1:t){
  if(i == 1){
    mu[i] <- rnorm(1, sd = 1)
  }else{
    mu[i] <- mu[i-1] + rnorm(1, sd = 5)
  }
  y[i] <- mu[i] + i + rnorm(1, sd = 1)
}

yd <- tibble(y, time = 1:t)

p1 <- ggplot(yd, aes(x = time, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

yd$resid <- resid(lm(y ~ time, data = yd))

p2 <- ggplot(yd, aes(x = time, y = resid)) + 
  geom_point() + geom_hline(yintercept = 0, color = "red")

grid.arrange(p1, p2, nrow = 1)

```


## Auto-correlation

The correlation between observations at any two time points is called the auto-correlation.

Our proposed SSM model has the following auto correlation structure.


\begin{align*}
corr(y_{it}, y_{i(t+\tau)}) = \frac{t\sigma^2_\eta}{\sqrt{\sigma^2_\varepsilon + t\sigma^2_\eta}\sqrt{\sigma^2_\varepsilon + (t+\tau)\sigma^2_\eta}}
\end{align*}

This is equivalent to a dynamic moving average covariance structure which is very flexible. If $\sigma^2_\eta = 0$ then auto-correlation is 0 and our proposed model boils down to a LMEM.

\begin{align*}
y_t &= \alpha_0 + X_t\beta
+ \varepsilon_t\\
\end{align*}


## Summary

Consider the model,

\begin{align*}
y_t &= \alpha_t + X_t\beta_t
+ \varepsilon_t\\
\alpha_t &= \alpha_{t-1} + \eta_t\\
\beta_t &= \beta_{t-1}
\end{align*}

We can think of $\alpha_t$ as the underlying cognitive state not accounted for by the baseline covariates $X$. 

The variable $\beta_t$ is the effect of the covariates $X_t$. It has the same interpretation as with a LMEM.


## Relation to State Space Model

We can rewrite the proposed model to fit the state space model as follows,

\begin{align*}
y_t = 
\begin{bmatrix}
I_n & X_t
\end{bmatrix}
\begin{bmatrix}
\alpha_t\\
\beta_t
\end{bmatrix}
+ \varepsilon_t\\
\begin{bmatrix}
\alpha_t\\
\beta_t
\end{bmatrix} = 
\begin{bmatrix}
I_{(n+p) \times (n+p)}
\end{bmatrix}
\begin{bmatrix}
\alpha_{t-1}\\
\beta_{t-1}
\end{bmatrix} + 
\begin{bmatrix}
\eta_t \\
0_{p \times 1}
\end{bmatrix}
\end{align*}



\begin{columns}
\begin{column}{0.35\textwidth}
\begin{itemize}
\item $F_t = \begin{bmatrix}I_n & X_t\end{bmatrix}$
\item $v_t = \varepsilon_t$
\item $w_t = \begin{bmatrix} \eta_t \\ 0_{p\times1} \end{bmatrix}$
\end{itemize}
\end{column}
\begin{column}{0.35\textwidth}
\begin{itemize}
\item $\mu_t = \begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix}$
\item $G_t = I_{(n+p) \times (n+p)}$
\end{itemize}
\end{column}
\end{columns}


## Kalman Filter

The Kalman filter is a recursive algorithm to estimate the unobserved states conditioned on the observed data (Kalman, 1960; Durbin and Koopman, 2012). Let $\hat \mu_{i|j} = E(\mu_i|y_{1:j})$ and $P_{i|j} = var(\mu_i|y_{1:j})$.

Predicted state: $\hat \mu_{t|t-1} = G_t \hat \mu_{t-1|t-1}$

Predicted state covariance: $P_{t|t-1} = G_tP_{t-1|t-1}G_t' + W$

Innovation covariance: $S_t = F_tP_{t|t-1}F_t' + V$

Kalman Gain: $K_t = P_{t|t-1}F_t'S^{-1}_t$

Innovation: $\tilde{f_t} = y_t - F_t \hat \mu_{t|t-1}$

Updated state estimate: $\hat \mu_{t|t} = \hat \mu_{t|t-1} + K_t \tilde f_t$

Updated state covariance: $P_{t|t} = (I- K_tF_t)P_{t|t-1}$

Updated innovation: $\tilde f_{t|t} = y_t - F_t \hat \mu_{t|t}$

## Kalman Smoother

Let $J_t = P_{t|t} G_{t+1}' + P^{-1}_{t+1|t}$. We can then calculate $E(\mu_t|y_{1:T})$ and $var(\mu_t|y_{1:T})$ using the following Kalman smoother equations.


\begin{align*}
E(\mu_t|y_{1:T}) = \hat \mu_{t|t} + J_t (\hat \mu_{t+1|T} - \hat \mu_{t+1|t})\\
var(\mu_t|y_{1:T}) = P_{t|t} - J_t G_{t+1} P_{t|t}
\end{align*}


## Setting Parameters

We assume $\mu_0 \sim N(u_0, P_0)$, however $u_0$ and $P_0$ are unknown.

* By initializing $u_0 = 0$ and $P_0 = \infty$ we are essentially putting a flat prior on $\mu_0$.
* It has been shown $\hat \mu_{0|T}$ and $P_{0|T}$ quickly converge to $u_0$ and $P_0$ respectively for even small $T$ (Kalman, 1960; Durbin and Koopman, 2012).

In our proposed model, $\mu_t = \begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix}$.

* $\hat\beta_{0|T}$ is then our estimate for $\beta$ and has variance covariance $P_{\hat\beta} = [P_{0|T}]_{(n+1):(n+p),(n+1):(n+p)}$.
* We can then use $\hat\beta_{0|T}$ and $P_{\hat\beta}$ for inference on $\beta$.
  * $\hat \beta \mysim N(\beta, P_{\hat\beta})$.

## Estimation of $\sigma^2_\varepsilon$ and $\sigma^2_\eta$

* We get proper estimates for $\beta$ given we have correctly specified our model, including $\sigma^2_\varepsilon$ and $\sigma^2_\eta$.
* The parameters $\sigma^2_\varepsilon$ and $\sigma^2_\eta$ are unknown, but can be estimated using Maximum Likelihood Estimation (MLE).

\begin{align*}
\ell(\sigma^2_\varepsilon, \sigma^2_\eta) = -\frac{np}{2}log(2\pi)-\frac{1}{2}\sum^t_{i=1}\big(log|\tilde S_i| + \tilde f_i` S_i^{-1} f_i)
\end{align*}

* To maximize the log-likelihood we used a Newton-Raphson method with a limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method (Liu and
Nocedal, 1989; Zhou and Li, 2007).




## Missing Data


If a subject is missing an observation at time $t$ we can set

* $y^* = W_ty_t$ where $W_t$ is a subset of rows of $I_n$ corresponding to those with observed data.
* $F^*_t = W_t F_t$
* $\varepsilon_t^* = W_t \varepsilon_t$

then carry out the same Kalman filter and smoother replacing $y$ with $y^*$, $Z$ with $Z^*$, and $\varepsilon_t^*$ with $\varepsilon_t$. Doing this modification still allows us to get the smoothed values for $\alpha_t$ and $\beta_t$.

## Computational Challenges

For each iteration of the kalman filter we must invert $var(Y_t|y_{1:(t-1)}) = S_t$.

* $S_t$ is non-sparse as calculating $var(Y_t|y_{1:(t-1)})$ is a function of $\beta_{t-1}$ which is shared between all observations.
* $S_t$ is an $n \times n$, so as $n$ increases there is an exponential increase in computation time.

## Solution 1: Partitioning

A solution to solving inversion computational inefficiencies is to partition:

* Partition the subjects into $k$ groups.
* Run the Kalman filter and smoother on each group independently to extract $\hat\beta_{0|T}^{(i)}$ and $P_{\beta}^{(i)}$ for $i$ in $1, ..., k$.
* Use the estimate $\bar\beta = \frac{\sum_{i=1}^k\hat\beta_{0|T}^{(i)}}{k}$.
  * $\bar \beta \sim N(\beta, \frac{\sum_{i=1}^kP_{\hat\beta^{(i)}}}{k^2})$


## Solution 2: Bayesian Gibb's Sampling Approach

* For the Bayesian approach we use a Gibb's sampler.
* Instead of calculating $\beta$ in the Kalman filter, we can estimate it separately.
* The model,

\begin{equation*}
\begin{aligned}
y_t &= \alpha_t + X_t \beta +\varepsilon_t\\
\alpha_t &= \alpha_{t-1} + \eta_t
\end{aligned}
\end{equation*}

## Gibb's Sampling

* Gibb's sampling is a method to gain an approximate sample from a posterior distribution for a given variable (Gelfand-Smith, 1990).
* It works by:
  * calculating the distribution of a variable conditioned on all other unknown variables, known as the posterior distribution.
  * sampling from the posterior distribution and assigning the new sample to the variable.
  * calculate the posterior of the next variable and continue to sample, update, and recalculate the other posteriors.
  * The process is commonly repeated thousands of times.
* We need to calculate the posterior for $\alpha_{1:T}, \beta, \sigma^2_{\varepsilon}, \sigma^2_\eta$.


## Posterior of $\alpha$

* Notice, if we are conditioning on $\beta$ for the posterior $\alpha_{1:T}|...$ then each $y_{it}$ is independent and we can run the Kalman filter chains independently.
* Let $y^*_t = y_t - X_t \beta$, then the model becomes
  
\begin{equation*}
\begin{aligned}
y^*_t &= \alpha_t +\varepsilon_t\\
\alpha_t &= \alpha_{t-1} + \eta_t
\end{aligned}
\end{equation*}

* We can then run a forward Kalman filter with a backward sampler to sample from the posterior of $\alpha_{1:T}$ (Fruhwirth-Schnatter, 1994)

## Posterior of $\beta$

* We let $\beta \sim N(\theta, \sigma^2_\beta)$
*  The posterior is $\beta|... \sim N(\Sigma^{-1}B, \sigma^2_\varepsilon\sigma^2_\beta\Sigma^{-1})$ where,
  * $B = \sigma^2_\beta\big(\sum^T_{t=1}y_t-\alpha_t\big)'X_t -\sigma^2_\varepsilon\theta$
  * $\Sigma = \big(\sigma^2_\beta\sum^T_{t=1}X_t'X_t\big)+\sigma^2_\varepsilon I_p$

## Posterior of $\beta$

For each iteration of the Gibb's sampler we must calculate, $\Sigma^{-1} = (\big(\sigma^2_\beta\sum^T_{t=1}X_t'X_t\big)+\sigma^2_\varepsilon I_p)^{-1}$. As $\sigma^2_\varepsilon$ is updated each iteration, $\Sigma^{-1}$ will be different for each iteration as well. However, $(\sigma^2_\beta\sum^T_{t=1}X_t'X_t\big)$ remains constant. By calculating the eigenvalue decomposition before the Gibb's sampler we can increase computation speed.


\begin{align*}
(\big(\sigma^2_\beta\sum^T_{t=1}X_t'X_t\big) + \sigma^{2(i)}_\varepsilon I) &=
( Q\Lambda Q' + \sigma^{2(i)}_\varepsilon I)\\
&=
( Q\Lambda Q' + \sigma^{2(i)}_\varepsilon QQ')\\
&=
Q(\Lambda + \sigma^{2(i)}_\varepsilon I)Q'\\
\text{then,}\\
(\big(\sigma^2_\beta\sum^T_{t=1}X_t'X_t\big)+ \sigma^{2(i)}_\varepsilon I)^{-1} &=
Q\big(1/(\Lambda + \sigma^{2(i)}_\varepsilon I)\big) Q'
\end{align*}



## Posterior of $\sigma^2_\varepsilon$ and $\sigma^2_\eta$

* Let,

\begin{equation*}
\begin{aligned}
\sigma^2_\eta &\sim IG(a_0/2, b_0/2)\\
\sigma^2_\varepsilon &\sim IG(c_0/2,d_0/2)
\end{aligned}
\end{equation*}

* Then 

\begin{equation*}
\begin{aligned}
\sigma^2_\eta|... &\sim IG(\frac{nT+a_0}{2}, \frac{\sum^T_{t=1} (\alpha_t-\alpha_{t-1})^2+b_0}{2})\\
\sigma^2_\varepsilon|... &\sim IG(\frac{nT+c_0}{2}, \frac{d_0 + \sum^T_{t=1}(y_t-X_t\beta-\alpha_t)^2}{2})
\end{aligned}
\end{equation*}


## The Gibbs Sampling Algorithm

1. Select prior parameters for $\theta, \sigma^2_\beta, a_0, b_0, c_0, d_0$.
2. Let $\beta^{(0)} = \theta$, $\sigma^{2(0)}_\eta = \frac{d_0/2}{1+c_0/2}$, and $\sigma^{2(0)}_\varepsilon = \frac{b_0/2}{1+a_0/2}$.
3. Run a forward-filtering backward sampling procedure as described above conditioning on $\beta^{i-1}, \sigma^{2(i-1)}_\eta, \sigma^{2(i-1)}_\varepsilon$  and set the samples equal to $\alpha^{(i)}$ for the $i^{th}$ iteration.
4. Sample $\sigma^{2*}_\eta$ from $IG(\frac{nT+a_0}{2}, \frac{\sum^T_{t=1} (\alpha^{(i)}_t-\alpha^{(i)}_{t-1})^2+b_0}{2})$ and set $\sigma^{2(i)}_\eta = \sigma^{2*}_\eta$.
5. Sample $\sigma^{2*}_\varepsilon$ from $IG(\frac{nT+c_0}{2}, \frac{d_0 + \sum^T_{t=1}(y_t-X_t\beta^{(i-1)}-\alpha_t^{(i)})^2}{2})$ and set $\sigma^{2(i)}_\varepsilon = \sigma^{2*}_\varepsilon$. 
6. Sample $\beta^*$ from $N(\Sigma^{-1}B, \sigma^2_\varepsilon\sigma^2_\beta\Sigma^{-1})$ where $\alpha = \alpha^{(i)}, \sigma^{2}_\eta = \sigma^{2(i)}_\eta, \sigma^{2}_\varepsilon=\sigma^{2(i)}_\varepsilon$ and set $\beta^{(i)} = \beta^*$.
7. Repeat steps 3-6 for $i$ in 1, 2, ..., M.


## Estimating $\beta$

* After throwing out a number of initial samples from the Gibb's sampler we can estimate $\beta$ by taking the mean of the posterior samples.
* We create a $95%$ credibility interval (as a pseudo-confidence interval) by calculating the $97.5^{th}$ and $2.5^{th}$ percentiles of the posterior draws.

## Simulation

* We sampled from the model,

\begin{equation*}
\begin{aligned}
y_t &= \alpha_t + t*X\beta + \varepsilon_t, & \varepsilon_t \sim N(0,\sigma_\varepsilon^2I_n)\\
\alpha_t &= \alpha_{t-1} +\eta_t, & \eta_t \sim N(0,\sigma_\eta^2I_n)
\end{aligned}
\end{equation*}

We simulated 100 subjects at 6 time points. $X$ was simulated from a $U(0,20)$ distribution and $\beta = (4 \ \ \ 2 \ -1)'$.

The variables $\sigma^2_\varepsilon$ and $\sigma^2_\eta$ varied between simulations. Recall, $\sigma^2_\eta = 0$ corresponds to a lmem with a random intercept.

We compared 95% CI coverage, CI length, and estimate variance between 1. LMEM with a random intercept, 2. LMEM with a random intercept and AR(1) error correlation structure, the matrix formulation of the state space model, the Bayesian estimated state space model, the a state space model partitioned into 2, 4, and 10 groups.



## Coverage


```{r}
# knitr::opts_chunk$set(eval = FALSE)
```


```{r, warning = FALSE}
flist <- list.files("Simulations/SimOutReform")
flist <- flist[grepl("Itout ",flist)]
B <- c(4, 2, -1)


Coverage <- sapply(flist, function(x){
  Itout <- readRDS(paste("Simulations/SimOutReform/",x, sep = "")) 
  Coverage <- apply(Itout$UCL >= B & Itout$LCL <= B, 2, mean) 
  
  
  Vars <- x %>% 
    gsub("Itout ", "", .) %>%
    gsub(".RDS", "", .) %>%
    str_split(" ") %>%
    .[[1]] %>%
    gsub("_", ".", .) %>%
    as.numeric()
    c(round(Vars, 1), round(Coverage, 3))
  
}) %>% 
  t()





colnames(Coverage) <- c("sigeps", "sigeta","LME", "AR(1)", "SSM", "Bayes", "Part2", "Part4", "Part10") 
# c("$\\sigma^2_\\varepsilon$", "$\\sigma^2_\\eta$","LME", "AR(1)", "SSM", "Bayes", "Part2", "Part4", "Part10") 

Coverage[,c("sigeps", "sigeta","LME", "AR(1)", "SSM", "Part2", "Part4", "Part10", "Bayes")] %>%
  data.frame() %>%
  rownames_to_column("SimScen") %>%
  mutate(sigeps = as.character(sigeps), sigeta = as.character(sigeta)) %>%
  mutate(
    sigeps = case_when(
      is.na(sigeps) ~ "1",
      TRUE ~ sigeps
    ),
    sigeta = case_when(
      is.na(sigeta) & grepl("Large", SimScen) ~ "$\\rho = 0.9$",
      is.na(sigeta) & grepl("Small", SimScen) ~ "$\\rho = 0.1$",
      TRUE ~ sigeta
    )
  ) %>%
  select(-SimScen) %>%
  kable(row.names = FALSE, booktabs = TRUE, escape = FALSE,
        col.names = c("$\\sigma^2_\\varepsilon$", "$\\sigma^2_\\eta$","LME", "AR(1)", "SSM", "Bayes", "Part2", "Part4", "Part10")) %>%
  add_header_above(c("Variance\nParameters" = 2, "Traditional\nMethods" = 2,"State Space Methods" = 5)) %>%
  kable_styling(latex_options = "striped")
```


## Bias

```{r, warning = FALSE, message = FALSE, fig.height=6}
# x <- flist[8]
outb <- lapply(flist, function(x){
   Vars <- x %>% 
    gsub("Itout ", "", .) %>%
    gsub(".RDS", "", .) %>%
    str_split(" ") %>%
    .[[1]] %>%
    gsub("_", ".", .) %>%
    as.numeric()

  Itout <- readRDS(paste("Simulations/SimOutReform/",x, sep = ""))
  Bias <- Itout$Estimate - B
  Var1Bias <- t(Bias[1,,])
  Var2Bias <- t(Bias[2,,])
  Var3Bias <- t(Bias[3,,])
  VarBias <- data.frame(rbind(Var1Bias, Var2Bias, Var3Bias))
  
  VarBias$Variable <- sort(rep(paste("B", 1:3, sep = ""), dim(Bias)[3]))
  
  
  
  colnames(VarBias) <-  c("LME", "LME AR(1)", "SS Orig", "SS Bayes", "SS Part 2", "SS Part 4", "SS Part 10",  "Variable") 
  
  VarBias %>%
    data.frame() %>%
    gather("Method", "Bias", `LME`:`SS.Part.10`) %>%
    mutate(Method = fct_case_when(
      Method == "LME" ~ "LME",
      Method == "LME.AR.1." ~ "LME AR(1)",
      Method == "SS.Orig" ~ "SS Original",
      Method == "SS.Bayes" ~ "SS Bayes",
      Method == "SS.Part.2" ~ "SS Part k = 2",
      Method == "SS.Part.4" ~ "SS Part k = 4",
      Method == "SS.Part.10" ~ "SS Part k = 10"
    ),
    sig2eps = Vars[1], sig2eta = Vars[2], Vars = paste("$\\sigma^2_{\\epsilon} = ", Vars[1], ", \ \\sigma^2_{\\eta} = ", Vars[2], "$", sep = ""),
    ) %>%
    mutate(
      Vars = case_when(
        grepl("Large", x) ~ "$\\sigma^2 = 1, \\rho = 0.9$",
        grepl("Small", x) ~ "$\\sigma^2 = 1, \\rho = 0.1$",
        TRUE ~ Vars
      )
    ) %>%
    # mutate(Vars = TeX(Vars)) %>%
    filter(Variable == "B1")

}) %>%
  do.call("rbind", .) %>%
  mutate(Method = factor(Method, levels = c("LME", "LME AR(1)", "SS Original", "SS Part k = 2", "SS Part k = 4", "SS Part k = 10", "SS Bayes"))) %>%
  mutate(Vars = as.factor(Vars)) %>%
  group_by(Vars, Method) %>%
  filter(!is.na(Bias)) %>%
  filter(Bias < quantile(Bias, .99) & Bias > quantile(Bias, .01)) %>%
  ungroup(Vars, Method)


levels(outb$Vars) <- TeX(levels(outb$Vars))
outb$Vars <- factor(outb$Vars, levels = levels(outb$Vars)[c(
  which(!grepl("rho", levels(outb$Vars))),
  which(grepl("rho", levels(outb$Vars)))
)])


ggplot(outb, aes(x = Method, y = Bias, fill = Method)) + 
  geom_violin() + 
  stat_summary(fun=median, geom="point", shape=18, size=5) +
  geom_hline(yintercept = 0 , color = "red") +
  facet_wrap((Vars) ~ ., scales = "free_y",
  labeller=label_parsed)+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())










```






## CI Length

```{r, fig.height=6, message = FALSE, warning = FALSE}


outci <- lapply(flist, function(x){
  Vars <- x %>% 
    gsub("Itout ", "", .) %>%
    gsub(".RDS", "", .) %>%
    str_split(" ") %>%
    .[[1]] %>%
    gsub("_", ".", .) %>%
    as.numeric()

  Itout <- readRDS(paste("Simulations/SimOutReform/",x, sep = ""))
  CIlen <- Itout$UCL -Itout$LCL
  CIlen1 <- t(CIlen[1,,])
  CIlen2 <- t(CIlen[2,,])
  CIlen3 <- t(CIlen[3,,])
  CIlenTot <- data.frame(rbind(CIlen1, CIlen2, CIlen3))

CIlenTot$Variable <- sort(rep(paste("B", 1:3, sep = ""), dim(CIlen)[3]))



colnames(CIlenTot) <-  c("LME", "LME AR(1)", "SS Orig", "SS Bayes", "SS Part 2", "SS Part 4", "SS Part 10",  "Variable") 

outci <- CIlenTot %>%
  data.frame() %>%
  gather("Method", "CI.Length", `LME`:`SS.Part.10`) %>%
  mutate(Method = fct_case_when(
    Method == "LME" ~ "LME",
    Method == "LME.AR.1." ~ "LME AR(1)",
    Method == "SS.Orig" ~ "SS Original",
    Method == "SS.Bayes" ~ "SS Bayes",
    Method == "SS.Part.2" ~ "SS Part k = 2",
    Method == "SS.Part.4" ~ "SS Part k = 4",
    Method == "SS.Part.10" ~ "SS Part k = 10"
  ),
    sig2eps = Vars[1], sig2eta = Vars[2], Vars = paste("$\\sigma^2_{\\epsilon} = ", Vars[1], ", \ \\sigma^2_{\\eta} = ", Vars[2], "$", sep = "")) %>%
    mutate(
      Vars = case_when(
        grepl("Large", x) ~ "$\\sigma^2 = 1, \\rho = 0.9$",
        grepl("Small", x) ~ "$\\sigma^2 = 1, \\rho = 0.1$",
        TRUE ~ Vars
      )
    ) %>%
  filter(Variable == "B1")
}) %>%
  do.call("rbind", .) %>%
  mutate(Method = factor(Method, levels = c("LME", "LME AR(1)", "SS Original", "SS Part k = 2", "SS Part k = 4", "SS Part k = 10", "SS Bayes")))%>%
  mutate(Vars = as.factor(Vars)) %>%
  group_by(Vars, Method) %>%
  filter(!is.na(CI.Length)) %>%
  filter(CI.Length < quantile(CI.Length, .99) & CI.Length > quantile(CI.Length, .01)) %>%
  ungroup(Vars, Method)

levels(outci$Vars) <- TeX(levels(outci$Vars))

outci$Vars <- factor(outci$Vars, levels = levels(outci$Vars)[c(
  which(!grepl("rho", levels(outci$Vars))),
  which(grepl("rho", levels(outci$Vars)))
)])

outci %>%
  # filter(!(Method %in% c("LME", "LME AR(1)"))) %>%
  ggplot(aes(x = Method, y = CI.Length, fill = Method)) + 
  geom_violin() + 
  stat_summary(fun=median, geom="point", shape=18, size=5) +
  facet_wrap((Vars) ~ ., scales = "free_y",
  labeller=label_parsed)+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())


```


## Key Take-aways

* The state space methods give unbiased estimates while maintaining near 0.95 coverage probability for the 95% CIs.
  * While the LME methods are unbiased, they do not maintain 0.95 coverage probability when auto-correlation is increased.
* If the number of subjects in each partition is reasonable compared to the number of coefficients to estimate, then partitioning returns very similar results to not partitioning.
* Of the state space models, the Bayesian method has the least amount of variability in the estimates, the smallest variability in the estimate variances, all while maintaining 0.95 coverage probability.
* However, the Bayesian method fails to converge when the data generation came from an AR(1) model.


## Note

* All the SSM models can handle non standard, unequally spaced, and continuous time observations.


## Future Steps

* Apply methods to existing data where the underlying distributions are unknown.
  * National Alzheimer's Coordinating Center (NACC).
  * Run power analysis (Similar to Alicia's).

















